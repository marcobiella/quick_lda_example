---
title: "Is it Sentient Today?"
author: "Marco Biella"
date: "2022-08-02"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE, cache = TRUE)

rm(list = ls())

library(tidyverse)
library(patchwork)
library(academictwitteR)
library(udpipe)
library(topicmodels)
library(tm)
library(tidytext)
library(stm)
library(parallel)
library(wordcloud)

#utils

#session information
sInfo <- sessionInfo()
#maximum number of cores to use
maxCores <- detectCores() - 1 #library(parallel) #required, package not loaded but functions are called
```

This project is developed explicitly to play around with NLP. Specifically, showcasing some easy LDA (Latent Dirichlet Allocation).  
This technique is described in this [paper](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf?ref=https://githubhelp.com). To interpret topic models, you might want to have a look at this other [paper](https://proceedings.neurips.cc/paper/2009/file/f92586a25bb3145facd64ab20fd554ff-Paper.pdf).  
Since this project is just a toy example, I decided to interpret the model based on intuition (you might want to be more rigorous).  

### The question: "Is lamda sentient today?"

I want to know what people (on Twitter) say (think?) about lamda today.  
So, I'm using Twitter's API to download people's tweets related to lamda or sentient ai and model their discourse using topic modeling.  
  
### First, get the data!  

I'm downloading English-only tweets (but no retweets) containing "lambda" or "sentient" in the form of a simple word or hashtag. I'm limiting my query to tweets posted today.  
Downloading tweets is very easy if you have a Twitter developer account. To do so, you can leverage the magic provided by the academictwitteR R package.  
  
The full query in Twitter jargon is this:  
"(#lamda OR lamda OR #sentient OR sentient) lang:en -is:retweet"

```{r}
source(file = "lamda_tweetScraping.R")
```

### Load the data  

The academictwitteR R package has a very handy function, "bind_tweets".  
By specifying the location of the downloaded tweets, the function load everything in a very tidy tabular format.  

```{r, echo = TRUE}
#retrieve tweets and format as tibble
lamda_dat <- bind_tweets(data_path = "lamda_data", output_format = "tidy")
```

The very tidy tibble contains the tweet itself along with many info (tweet id, timestamp, how many times it was re-tweeted, etc...).  

```{r}
lamda_dat <- lamda_dat %>%
  dplyr::select(
    tweet_id, created_at, retweet_count, like_count, text,
    conversation_id, possibly_sensitive, in_reply_to_user_id, lang, 
    source, user_created_at, user_protected, user_verified, 
    quote_count, user_tweet_count, user_list_count, author_id,
    user_followers_count, user_following_count, sourcetweet_type, sourcetweet_id, 
    sourcetweet_text, sourcetweet_lang, sourcetweet_author_id
  )
lamda_dat
```

### Formatting

It might be convenient to run some mild pre-processing (dealing with datetime is always painful).  
Plus, I want to extract information such as the hour and minute of the timestamp in two different variables.  

```{r, echo = TRUE}
#format and clean
lamda_dat <- lamda_dat %>%
  mutate(
    #date in date format
    created_at = gsub(pattern = "T", replacement = " ", x = created_at), #"T" to space
    created_at = gsub(pattern = "\\.(.*)Z", replacement = "", x = created_at), #remeove everything between "." and "Z"
    created_at = as.POSIXct(x = created_at, format = "%Y-%m-%d %H:%M:%S", tz = "GMT"),
    #extract day
    created_day = as.POSIXct(x = gsub(pattern = " (.*)$", replacement = "", x = as.character(created_at)),
                             format = "%Y-%m-%d", tz = "GMT"),
    #extract time
    created_time = gsub(pattern = "^(.*) ", replacement = "", x = as.character(created_at)),
    #extract hour
    created_hour = gsub(pattern = "^(.*) ", replacement = "", x = as.character(created_at)),
    created_hour = gsub(pattern = ":(.*)$", replacement = "", x = created_hour),
    created_hour = as.numeric(created_hour),
    #extract minute
    created_minute = gsub(pattern = "^(.*) ", replacement = "", x = as.character(created_at)),
    created_minute = gsub(pattern = "^[0-9][0-9]:|:[0-9][0-9]$", replacement = "", x = created_minute),
    created_minute = as.numeric(created_minute),
    #extract weekday
    created_weekDay = weekdays(x = created_at),
    #create day bin (15 min)
    created_dayBin = ((created_hour * 60) + created_minute) / 15 %>% round(digits = 1)
  )
```

### Quick summary  

The first thing I'd like to do is to summarize the tweets corpus.  
Just checking how many tweets are there, how many unique users, and so on...  

```{r}
#corpus summary
print(paste("unique tweets:", length(unique(lamda_dat$tweet_id)))) #unique tweets
print(paste("unique users:", length(unique(lamda_dat$author_id)))) #unique users
print(paste("unique conversations:", length(unique(lamda_dat$conversation_id)))) #unique conversations
```

### Quick glance at the time distribution  

The second thing I'd like to do with these data is looking at their time distribution.  
You know, just to have a look at peak time and so on... The red dashed lines isolate generic "working hours". 
And then, a fancy "clock plot"! Why not!?  

```{r, echo = TRUE}
linPlot <- ggplot(data = lamda_dat,
       aes(x = created_at)) +
  geom_histogram(bins = 24, color = "black", fill = NA) +
  geom_vline(xintercept = as.POSIXct("2022-08-02 08:00:00 GMT", tz = "GMT"),
             lty = "dashed", color = "red") +
  geom_vline(xintercept = as.POSIXct("2022-08-02 18:00:00 GMT", tz = "GMT"),
             lty = "dashed", color = "red") +
  labs(x = "date / time") +
  ggtitle(label = "tweets frequency - timeseries")
circularPlot <- ggplot(data = lamda_dat,
       aes(x = created_hour %>% as.numeric(),
           group = created_day)) +
  geom_histogram(bins = 24, color = "black", fill = NA) +
  geom_vline(xintercept = c(8, 18), lty = "dashed", color = "red") +
  labs(x = "hour") +
  coord_polar(start = -(pi / 20)) +
  ggtitle(label = "tweets frequency - on the clock")
linPlot + circularPlot
```

### Some real text preprocessing  

Let's start with something easy:  

- Remove emojis  
- Remove new lines   
- Remove trailing and leading spaces  

```{r, echo = TRUE}
#clean text vector
lamda_dat <- lamda_dat %>%
  mutate(
    #copi text vector
    textVector = text, 
    #remove emojis
    textVector = gsub(pattern = "\\p{So}", replacement = "", x = textVector, perl = TRUE),
    #remove new line (using spaces, will be handeld by str_squish)
    textVector = gsub(pattern = "\n", replacement = " ", x = textVector), 
    #remove trailing/leading and multiple spaces
    textVector = str_squish(string = textVector)
    )
```

Then, store the text of the tweets in a single vector (textVector), extract all unique characters, and create a new vector containing only the allowed characters (letters and number)  
This vector of valid characters will be handy later.  

```{r, echo = TRUE}
#store text in a vector
textVector <- lamda_dat$text
#extract symbols vector (useful later)
symbVct <- str_split(string = textVector, pattern = "") %>% unlist() %>% unique()
symbVct <- symbVct[!(symbVct %>% grepl(pattern = "[0-9]|[a-z]|[A-Z]"))]
```

To conveniently process text data, I use the udipipe (universal dependency) R package.  
It has many convenient functions (i.e., for lemmatization) and several well maintained pre-trained models based on manually annotated textual datasets.  
Moreover, these models are available in several languages. This time however, the whole corpus is in English.  
Now, it's useful to leverage one of the pre-trained model created from Twitter data.  
So, let's download the model first, and store it into an R object.  

```{r, echo = TRUE}
#download model
engModelInfo <- udpipe_download_model(language = "english", overwrite = FALSE)
#load model
engModel <- udpipe_load_model(file = engModelInfo$file_model)
```

The udpipe_annotate function conveniently annotates the corpus.  
The code below extract different parts of speech. It splits every tweet into tokens (i.e., single words) and lemmas (i.e., "haven't" becomes "have" + "not"), and tags everything with the proper part of speech (i.e., nouns, pronouns, verbs, etc...).  
A column storing the document id (doc_id) is created as well as columns storing paragraph or sentence id.  
For this project, I'll work using lemmas (which convey meaning).

```{r, echo = TRUE}
#annotate
annotatedCorpus <- udpipe_annotate(object = engModel, x = textVector, trace = 1000)
#format
annotatedCorpus <- as_tibble(annotatedCorpus)
```

### Text preprocessing  

Now, it is time for some heavy preprocessing!  
The code below removes punctuation, stopwords (functional words that do not convey meaning), invalid characters (see the symbVct vector above), and other useless words (i.e., "https://" for links).  
Obviously, the keyword for the query are removed.  

```{r, echo = TRUE}
#text cleaning
annotatedCorpus <- annotatedCorpus %>%
  #to lower
  mutate(
    token = tolower(token),
    lemma = tolower(lemma)) %>%
  dplyr::filter(!is.na(lemma)) %>% #remove empty lemmas
  dplyr::filter(!(upos == "PUNCT")) %>% #remove punctuation
  #remove "'" from lemma and tokens
  mutate(token = gsub(pattern = "'", replacement = "", x = token),
         lemma = gsub(pattern = "'", replacement = "", x = lemma)) %>%
  #remove italian stopwords
  dplyr::filter(!(token %in% stopwords::stopwords(language = "en"))) %>%
  dplyr::filter(!(lemma %in% stopwords::stopwords(language = "en"))) %>%
  #remove wymbols (except # and @)
  dplyr::filter(!(token %in% symbVct)) %>%
  dplyr::filter(!(lemma %in% symbVct)) %>%
  #remove empty token\lemma
  dplyr::filter(token != "") %>%
  dplyr::filter(lemma != "") %>%
  #remove links
  dplyr::filter(!grepl(pattern = "https://", x = token)) %>%
  dplyr::filter(!grepl(pattern = "https://", x = lemma))
#text manual override (keywords from query)
annotatedCorpus <- annotatedCorpus %>%
  mutate(lemma = gsub(pattern = "#lamda|#sentient|lamda|sentient", replacement = NA, x = lemma)) %>%
  dplyr::filter(!is.na(lemma))
```

### Exploration  

Then, let's have a look at the most frequent words.  
These are the most frequent words, used at least 50 times.  

```{r}
#words count
wordFreq <- annotatedCorpus$lemma %>%
  table() %>% 
  as_tibble() %>%
  rename("lemma" = ".", "freq" = "n") %>%
  arrange(desc(freq))
wordFreq %>%
  mutate(lemma = fct_relevel(as.factor(wordFreq$lemma), rev(wordFreq$lemma), after = Inf)) %>%
  dplyr::filter(freq > 50) %>%
  ggplot(aes(y = freq, x = lemma)) + geom_point() + coord_flip() +
  labs(title = "words by frequency", y = "frequency", x = "word")
```

And then, let's create a term frequency-inverse document frequency.  
This gives us a measure of the relative importance of every single word within a document.  

```{r, echo = TRUE}
#tf_idf
corpus_tfIdf <- annotatedCorpus %>%
  dplyr::select(doc_id, lemma) %>%
  group_by(doc_id, lemma) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  bind_tf_idf(term = lemma, document = doc_id, n = count) %>%
  arrange(desc(tf_idf))
corpus_tfIdf
```

Finally, prior to diving into topic modeling, the annotated corpus should be transformed in a document/term matrix (excluding too rare words, below 50 instances).  

```{r, echo = TRUE}
#creates matrix
#document term frequency
dtf <- document_term_frequencies(x = annotatedCorpus,
                                 document = "doc_id", term = "lemma")
#document-term matrix
dtm <- document_term_matrix(x = dtf)
#remove non frequent words
dtm <- dtm_remove_lowfreq(dtm = dtm, minfreq = 50)
```

### Topic Modeling

This technique requires the number of topics to be provided a-priori. However, this number can be estimated trying several values and picking the one that provides the best value for some evaluation metrics (i.e., Semantic Coherence).  
The searchK function does exactly this. It test several number of topics (the K argument) and returns performance metrics for each value.  

```{r, echo = TRUE}
#probe potential values of k
kSearch <- searchK(documents = asSTMCorpus(documents = dtm)$documents, #documents
                   vocab = asSTMCorpus(documents = dtm)$vocab, #vocabulary
                   data = asSTMCorpus(documents = dtm)$data, #metadata (no data in this case)
                   K = c(10, 15, 20, 25, 30, 35), #potential values of k
                   cores = maxCores, #number of cores parallelization
                   proportion = .2, #proportion of held-out samples
                   heldout.seed = 42)
#visualize search k
plot(kSearch)
```

Looking at the plots, 30 topics seems like a reasonable number.  
Residuals, Held-Out Likelihood, and Semantic Coherence have decent values.  

### Fitting the Model  

Based on the K value obtained in the previous step, it's now time to fit the model!  
A Latent Dirichlet Allocation model is really easy to fit (asssuming you know how many topics are there!).  
I used K=30 (from the previous step), and alpha=.10. The alpha parameter refers to the Dirichlet distribution. A relatively low value of alpha promotes *"quasi-sparse"* allocation probabilities. It means that each tweet will have a relatively high probability of being assigned to one topic and relatively low probability of being assigned to the remaining topics (LDA topic models are mixed-membership models!).

```{r, echo = TRUE}
#train best model
ldaModel <- LDA(x = dtm,
                #best number of topics from cross-validation
                k = 30,
                method = "Gibbs",
                control = list(
                  #dirichlet alpha parameter from cross-validation
                  alpha = .10, #keep first topic pro high, and second low
                  burnin = 2000, best = TRUE, keep = 50))
```

After the model is fitted, the following custom functions can be used to extract the most important keywords, some tweets as topic exemplars, and create wordclouds.  
These techniques might help in interpreting the model. However, more scientific approaches such as word-intrusion or topic-intrusion should be preferred.  

```{r, echo = TRUE}
#utility functions
createKeywordCloud <- function(topic_id, w = 10, model = ldaModel){
  #create wordcloud of w keywords for topic_id by posterior
  words = posterior(model)$terms[topic_id, ]
  topwords = head(sort(words, decreasing = TRUE), n = w)
  return(wordcloud(words = names(topwords), topwords#, scale = c(1.75, 1.75)
                   ))
}
keywordPosterior <- function(topic_id, w = 10, model = ldaModel){
  #extract top w keywords for topic_id ordered by posterior
  words = posterior(model)$terms[topic_id, ]
  topwords = head(sort(words, decreasing = TRUE), n = w)
  return(topwords)
}
extractExemplarDoc <- function(topic_id, exemplars = 10, model = ldaModel){
  #extract (id and posterior of the) most representative documents (n = exemplars) of a given topic (topic_id)
  words = posterior(model)$terms[, topic_id]
  ret = names(head(sort(words, decreasing = TRUE), n = exemplars))
  ret = as.numeric(ret)
  return(ret)
}
```

Let's extract the top 10 keywords for each topic.  

```{r}
#combine terms and posteriors
bestKeywordsPosterior <- terms(x = ldaModel, k = 10) %>% as_tibble()
colnames(bestKeywordsPosterior) <- gsub(pattern = " ", replacement = "", x = colnames(bestKeywordsPosterior))
bestKeywordsPosterior #visualize
```

Keywords importance is estimated using the posterior probability from the LDA model.  

```{r}
for(indx in 1:30){ #attach posterior to best keyword table
  bestKeywordsPosterior[, paste0("posterior", indx)] <- unname(keywordPosterior(topic_id = indx, w = 10, model = ldaModel))
}; rm(indx)
bestKeywordsPosterior <- bestKeywordsPosterior %>% #reshape and merge
  mutate(rank = 1:n()) %>%
  gather(key = "key", value = "value", -rank) %>% 
  mutate(topic = gsub(pattern = "Topic|posterior", replacement = "", x = key),
         parameter = tolower(gsub(pattern = "[0-9]|[0-9][0-9]|[0-9][0-9][0-9]", replacement = "", x = key))) %>%
  dplyr::select(-key)
bestKeywordsPosterior <- merge(x = bestKeywordsPosterior %>%
                                 dplyr::filter(parameter == "topic") %>%
                                 rename(keyword = value) %>%
                                 dplyr::select(-parameter),
                               y = bestKeywordsPosterior %>%
                                 dplyr::filter(parameter == "posterior") %>%
                                 rename(posterior = value) %>%
                                 mutate(posterior = as.numeric(posterior)) %>%
                                 dplyr::select(-parameter),
                               by.x = c("topic", "rank"),
                               by.y = c("topic", "rank")) %>%
  as_tibble() %>%
  mutate(topic = as.integer(topic)) %>%
  arrange(topic, rank)
bestKeywordsPosterior
```

And visualized graphically.  

```{r}
#visual posterior
ggplot(data = bestKeywordsPosterior %>%
         mutate(keyword = reorder_within(keyword, posterior, topic)),
       aes(x = keyword, y = posterior)) +
  geom_point() +
  facet_wrap(. ~ topic, scales = "free_y") +
  coord_flip()
```

Similarly, extracting tweet exemplars for each topic can help making sense of the model.  
(note: the original tweet is displayed instead of the preprocessed version)  

```{r, echo = TRUE}
#extract best exemplars for each topic
exemplarPerTopic <- c()
for(indx in 1:30){ #for each topic, extract some exemplar
  #extract text index
  topic_exemplars <- extractExemplarDoc(topic_id = indx, exemplars = 10, model = ldaModel) #extract exemplars index
  #extract exemplar and attach topic id
  tmpExemplar <- cbind("topic" = indx,
                       "exemplar" = textVector[topic_exemplars])
  #store tmpExemplar into exemplarPerTopic
  exemplarPerTopic <- rbind(exemplarPerTopic, tmpExemplar)
}; rm(indx, tmpExemplar)
exemplarPerTopic <- exemplarPerTopic %>% #format properly
  as_tibble() %>%
  mutate(topic = as.integer(topic)) %>%
  arrange(topic)
head(exemplarPerTopic, n = 20)
```

Wordclouds are generally a messy visualization, but sometimes they can be useful.  
The most important words per topic are visualized in the cloud.  

```{r}
par(mfrow = c(6, 5), mar = c(1, 1, 1, 1))
for(indx in 1:30){
  createKeywordCloud(topic_id = indx, w = 15, model = ldaModel)
  title(main = paste0("topic", indx),
        x = 0, y = -1,
        col.main = "dark red")
}; rm(indx)
```

